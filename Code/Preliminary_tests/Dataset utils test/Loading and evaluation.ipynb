{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n",
      "There are 51848 images in the training set\n",
      "There are 17091 annotations in the training set\n",
      "{'height': 640, 'width': 1024, 'daytime': 'night', 'file_name': '58c58331bc2601370015aa43.png', 'id': 7027652, 'recordings_id': 36.0, 'timestamp': 6307696984}\n",
      "File path: /home/test/data/nightowls/nightowls_validation/58c58331bc2601370015aa43.png\n",
      "Number of annotations in the image: 3\n",
      "{'occluded': False, 'difficult': False, 'bbox': [179, 233, 41, 119], 'id': 7014321, 'category_id': 1, 'image_id': 7027652, 'pose_id': 4, 'tracking_id': 7001032, 'ignore': 0, 'area': 4879, 'truncated': False}\n",
      "{'occluded': False, 'difficult': False, 'bbox': [60, 223, 27, 90], 'id': 7014322, 'category_id': 1, 'image_id': 7027652, 'pose_id': 2, 'tracking_id': 7001033, 'ignore': 0, 'area': 2430, 'truncated': False}\n",
      "{'occluded': False, 'difficult': False, 'bbox': [104, 228, 31, 97], 'id': 7014323, 'category_id': 1, 'image_id': 7027652, 'pose_id': 2, 'tracking_id': 7001034, 'ignore': 0, 'area': 3007, 'truncated': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred class:\n",
      "['person', 'bus', 'person', 'person', 'person', 'handbag', 'handbag', 'backpack', 'handbag', 'handbag', 'person', 'person', 'bus', 'person', 'handbag', 'backpack', 'suitcase', 'backpack', 'backpack', 'bench', 'handbag', 'person', 'truck', 'backpack', 'handbag', 'truck', 'backpack', 'train', 'car', 'person', 'handbag', 'refrigerator', 'person', 'bus', 'refrigerator', 'person', 'person', 'person', 'truck']\n",
      "pred_boxes\n",
      "[[(173.7489, 236.49275), (231.31038, 341.77567)], [(213.97305, 156.19266), (495.48972, 319.37366)], [(106.12935, 231.77364), (136.71959, 320.31104)], [(53.104748, 227.83263), (79.708824, 312.02423)], [(140.7585, 236.40726), (154.66072, 267.28928)], [(120.90885, 289.28326), (138.58212, 311.96783)], [(179.22858, 260.0619), (207.45108, 289.26233)], [(120.253075, 247.7241), (134.23001, 269.40765)], [(126.41931, 249.55669), (136.64928, 268.05908)], [(178.40186, 270.61887), (202.45409, 288.50854)], [(65.36184, 232.01225), (82.295876, 298.00974)], [(39.896633, 224.09863), (79.97405, 350.4913)], [(30.775366, 76.300575), (514.08905, 332.0979)], [(139.94826, 235.9394), (154.82011, 252.29893)], [(122.02233, 247.27917), (135.27199, 272.04562)], [(185.61626, 246.50562), (210.61058, 279.0358)], [(121.64036, 291.68176), (138.1665, 313.37982)], [(126.328224, 249.18115), (136.74608, 268.12213)], [(113.19811, 247.66008), (133.1504, 276.02176)], [(433.62668, 15.78081), (819.8887, 469.4034)], [(187.68811, 284.85663), (213.4506, 309.9885)], [(261.8978, 207.2863), (303.91843, 255.51135)], [(223.94029, 149.27344), (496.9977, 322.94504)], [(60.61217, 234.25682), (82.235405, 269.36453)], [(108.149086, 251.88904), (122.83874, 279.82214)], [(440.14688, 55.63756), (815.3515, 413.98715)], [(179.09807, 259.15427), (208.39783, 288.08322)], [(0.0, 122.87682), (54.650814, 339.6127)], [(224.6376, 147.56453), (498.71432, 324.34494)], [(257.19034, 206.26375), (305.33398, 294.03772)], [(104.81476, 250.15025), (116.21818, 275.13086)], [(481.0711, 64.379234), (811.56903, 394.05142)], [(118.75584, 241.6499), (136.91519, 299.1511)], [(288.42703, 53.80005), (826.3009, 401.22525)], [(1.7453171, 135.64949), (51.060223, 341.77786)], [(242.25285, 247.20808), (252.61507, 283.37558)], [(52.06431, 228.16295), (68.6572, 299.6607)], [(55.59422, 227.41577), (73.73515, 250.81041)], [(0.0, 127.516106), (53.583363, 344.25104)]]\n",
      "pred_score\n",
      "[0.99919945, 0.9855112, 0.98283273, 0.93397367, 0.8051181, 0.51825756, 0.3360107, 0.31491554, 0.19924353, 0.19141676, 0.17201163, 0.1531161, 0.15231651, 0.14735813, 0.14145638, 0.14142844, 0.13602875, 0.13048902, 0.11760091, 0.11595694, 0.11019498, 0.10734011, 0.09420266, 0.08951188, 0.08780695, 0.08592723, 0.08492838, 0.07518663, 0.070670694, 0.069550715, 0.06686445, 0.062088687, 0.061703, 0.05998407, 0.05833218, 0.055995632, 0.054906696, 0.053391688, 0.051658902]\n",
      "[(173.7489, 236.49275), (231.31038, 341.77567)]\n",
      "person\n",
      "{'image_id': 7027652, 'category_id': 1, 'score': 0.99919945, 'bbox': [173.7489, 236.49275, 57.561478, 105.28291]}\n",
      "[(213.97305, 156.19266), (495.48972, 319.37366)]\n",
      "bus\n",
      "{'image_id': 7027652, 'category_id': 1, 'score': 0.9855112, 'bbox': [213.97305, 156.19266, 281.51666, 163.181]}\n",
      "[(106.12935, 231.77364), (136.71959, 320.31104)]\n",
      "person\n",
      "{'image_id': 7027652, 'category_id': 1, 'score': 0.98283273, 'bbox': [106.12935, 231.77364, 30.59024, 88.5374]}\n",
      "[(53.104748, 227.83263), (79.708824, 312.02423)]\n",
      "person\n",
      "{'image_id': 7027652, 'category_id': 1, 'score': 0.93397367, 'bbox': [53.104748, 227.83263, 26.604076, 84.191605]}\n",
      "[(140.7585, 236.40726), (154.66072, 267.28928)]\n",
      "person\n",
      "{'image_id': 7027652, 'category_id': 1, 'score': 0.8051181, 'bbox': [140.7585, 236.40726, 13.902222, 30.882019]}\n",
      "[(120.90885, 289.28326), (138.58212, 311.96783)]\n",
      "handbag\n",
      "{'image_id': 7027652, 'category_id': 1, 'score': 0.51825756, 'bbox': [120.90885, 289.28326, 17.673271, 22.68457]}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from coco import COCO\n",
    "\n",
    "# import necessary libraries\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# get the pretrained model from torchvision.models\n",
    "# Note: pretrained=True will get the pretrained weights for the model.\n",
    "# model.eval() to use the model for inference\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Class labels from official PyTorch documentation for the pretrained model\n",
    "# Note that there are some N/A's \n",
    "# for complete list check https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "# we will use the same list for this notebook\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "\n",
    "def get_prediction(img_path, threshold):\n",
    "  \"\"\"\n",
    "  get_prediction\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - threshold - threshold value for prediction score\n",
    "    method:\n",
    "      - Image is obtained from the image path\n",
    "      - the image is converted to image tensor using PyTorch's Transforms\n",
    "      - image is passed through the model to get the predictions\n",
    "      - class, box coordinates are obtained, but only prediction score > threshold\n",
    "        are chosen.\n",
    "    \n",
    "  \"\"\"\n",
    "  img = Image.open(img_path)\n",
    "  transform = T.Compose([T.ToTensor()])\n",
    "  img = transform(img)\n",
    "  img.to(device)\n",
    "  pred = model([img])\n",
    "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
    "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
    "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "\n",
    "  print('pred class:')\n",
    "  print(pred_class)\n",
    "  print('pred_boxes')\n",
    "  print(pred_boxes)\n",
    "  print('pred_score')\n",
    "  print(pred_score)\n",
    "\n",
    "  pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n",
    "  pred_boxes = pred_boxes[:pred_t+1]\n",
    "  pred_class = pred_class[:pred_t+1]\n",
    "  pred_score = pred_score[:pred_t+1]\n",
    "\n",
    "  \n",
    "    \n",
    "  return pred_boxes, pred_class, pred_score\n",
    "  \n",
    "\n",
    "\n",
    "def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
    "  \"\"\"\n",
    "  object_detection_api\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - threshold - threshold value for prediction score\n",
    "      - rect_th - thickness of bounding box\n",
    "      - text_size - size of the class label text\n",
    "      - text_th - thichness of the text\n",
    "    method:\n",
    "      - prediction is obtained from get_prediction method\n",
    "      - for each prediction, bounding box is drawn and text is written \n",
    "        with opencv\n",
    "      - the final image is displayed\n",
    "  \"\"\"\n",
    "  boxes, pred_cls = get_prediction(img_path, threshold)\n",
    "  img = cv2.imread(img_path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  for i in range(len(boxes)):\n",
    "    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n",
    "    cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
    "  plt.figure(figsize=(20,30))\n",
    "  plt.imshow(img)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.show()\n",
    "\n",
    "# path to the ground truth annotation and the data\n",
    "annFile = '/home/test/data/nightowls/nightowls_validation.json'\n",
    "image_directory = '/home/test/data/nightowls/nightowls_validation'\n",
    "\n",
    "# load the annotations and the list of images\n",
    "cocoGt = COCO(annFile)\n",
    "imgIds = sorted(cocoGt.getImgIds())\n",
    "print('There are %d images in the training set' % len(imgIds))\n",
    "annotations = cocoGt.getAnnIds()\n",
    "print('There are %d annotations in the training set' % len(annotations))\n",
    "\n",
    "import json\n",
    "\n",
    "# get a random image and its path\n",
    "# im_id = imgIds[random.randint(0, len(imgIds))] \n",
    "im_id = 7027652 # bus with three persons: 7027652\n",
    "image = cocoGt.loadImgs(ids=im_id)[0]\n",
    "file_path = path.join(image_directory, image['file_name'])\n",
    "\n",
    "\n",
    "# get the annotations of the images\n",
    "anns = cocoGt.getAnnIds([image['id']])\n",
    "\n",
    "print(image)\n",
    "print('File path: ' + str(file_path))\n",
    "print('Number of annotations in the image: ' + str(len(anns)))\n",
    "\n",
    "# draw the annotations on the image\n",
    "img=mpimg.imread(file_path)\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(img)\n",
    "\n",
    "for ann_id in anns:\n",
    "    ann = cocoGt.loadAnns(ids=ann_id)[0]\n",
    "    print(ann)\n",
    "    bbox = ann['bbox']\n",
    "    rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=2,edgecolor='g',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "#print\n",
    "plt.show()\n",
    "\n",
    "#object_detection_api(file_path, threshold=0.8)\n",
    "boxes, pred_cls, pred_score = get_prediction(file_path, threshold=0.5)\n",
    "\n",
    "\n",
    "for i in range(len(boxes)):\n",
    "    print(boxes[i])\n",
    "    print(pred_cls[i])\n",
    "    \n",
    "    jsonresult = {\n",
    "      \"image_id\": im_id,\n",
    "      \"category_id\": 1,\n",
    "      \"score\": pred_score[i],\n",
    "      \"bbox\": []\n",
    "    }\n",
    "    jsonresult['bbox'].append(boxes[i][0][0])\n",
    "    jsonresult['bbox'].append(boxes[i][0][1])\n",
    "    jsonresult['bbox'].append(boxes[i][1][0] - boxes[i][0][0])\n",
    "    jsonresult['bbox'].append(boxes[i][1][1] - boxes[i][0][1])\n",
    "    \n",
    "    print(jsonresult)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=7.60s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.47s).\n",
      " Average Miss Rate  (MR) @ Reasonable         [ IoU=0.50      | height=[50:10000000000] | visibility=[0.65:10000000000.00] ] = 21.54%\n",
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=4.07s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=6.22s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.30s).\n",
      " Average Miss Rate  (MR) @ Reasonable_small   [ IoU=0.50      | height=[50:75] | visibility=[0.65:10000000000.00] ] = 32.13%\n",
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=7.18s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.48s).\n",
      " Average Miss Rate  (MR) @ Reasonable_occ=heavy [ IoU=0.50      | height=[50:10000000000] | visibility=[0.20:0.65] ] = 73.01%\n",
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=8.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.47s).\n",
      " Average Miss Rate  (MR) @ All                [ IoU=0.50      | height=[20:10000000000] | visibility=[0.20:10000000000.00] ] = 35.61%\n"
     ]
    }
   ],
   "source": [
    "from coco import COCO\n",
    "from eval_MR_multisetup import COCOeval\n",
    "\n",
    "# Ground truth\n",
    "annFile = '/home/test/data/nightowls/nightowls_validation.json'\n",
    "\n",
    "# Detections\n",
    "resFile = '../sample-Faster-RCNN-nightowls_validation.json'\n",
    "\n",
    "## running evaluation\n",
    "res_file = open(\"results.txt\", \"w\")\n",
    "for id_setup in range(0,4):\n",
    "    cocoGt = COCO(annFile)\n",
    "    cocoDt = cocoGt.loadRes(resFile)\n",
    "    imgIds = sorted(cocoGt.getImgIds())\n",
    "    cocoEval = COCOeval(cocoGt,cocoDt,'bbox')\n",
    "    cocoEval.params.imgIds  = imgIds\n",
    "    cocoEval.evaluate(id_setup)\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize(id_setup,res_file)\n",
    "\n",
    "res_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
