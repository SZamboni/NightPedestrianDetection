{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "My original Dataset\n",
    "'''\n",
    "'''\n",
    "We create a dataset capable of loading the images in the nightowl dataset, then\n",
    "a network to evaluate them and write the result in a json file with the right format\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.nn import utils\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "from coco import COCO\n",
    "\n",
    "\n",
    "'''\n",
    "Custom Dataset in order to load the images with the bbox and the labels\n",
    "'''\n",
    "class MyCustomDataset(torch.utils.data.Dataset):\n",
    "# __init__ function is where the initial logic happens like reading a csv\n",
    "    def __init__(self, file_path, images_path):\n",
    "\n",
    "        # Create the transformation that will be applied\n",
    "        #self.transform_to_apply = transforms.Compose([ transforms.Resize(image_size),\n",
    "            #transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "        self.transform_to_apply = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "        self.images_path = images_path\n",
    "        \n",
    "        # get the dataset, the annotiations and the imagesID\n",
    "        self.cocovar = COCO(file_path)\n",
    "        self.imgIds = sorted(self.cocovar.getImgIds())\n",
    "\n",
    "        self.imgs_count = len(self.imgIds)\n",
    "\n",
    "    # __getitem__ function returns the data and labels. This function is called from dataloader\n",
    "    # the format of return is a list containing the image with the applied transformation\n",
    "    # and another list containing for every bbox [x,y,width,height,category_id]\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # get the image id and its filename\n",
    "        img_id = self.imgIds[index]\n",
    "        cocoimg = self.cocovar.loadImgs(ids=img_id)[0]\n",
    "        img_filename = cocoimg['file_name']\n",
    "\n",
    "        # Open image and apply the transformation\n",
    "        img_as_img = Image.open(os.path.join(self.images_path,img_filename))\n",
    "        img = self.transform_to_apply(img_as_img)\n",
    "\n",
    "        #load the ids of the annotations for the images\n",
    "        image_annotations = self.cocovar.getAnnIds(img_id)\n",
    "        \n",
    "        anns_to_ret = []\n",
    "        \n",
    "        # for every annotation\n",
    "        for ann_id in image_annotations:\n",
    "            #load all the annotation data\n",
    "            ann = self.cocovar.loadAnns(ids=ann_id)[0]\n",
    "            data = []\n",
    "            for i in ann['bbox']:\n",
    "                data.append(i)\n",
    "            data.append(ann['category_id'])\n",
    "            # data will be: [x,y,width,height,category_id]\n",
    "            \n",
    "            anns_to_ret.append(data)\n",
    "\n",
    "        return ( img, img_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imgs_count\n",
    "\n",
    "# Function that will create the dataset and the dataloader and return the dataloader\n",
    "def getDataLoader(file_path,images_path,batch_size,num_workers):\n",
    "\n",
    "    dataset = MyCustomDataset(file_path,images_path)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "def get_prediction(img_path, threshold,model):\n",
    "    img = Image.open(img_path) # Load the image\n",
    "    transform = transforms.Compose([transforms.ToTensor()]) # Defing PyTorch Transform\n",
    "    img = transform(img) # Apply the transform to the image\n",
    "    pred = model([img]) # Pass the image to the model\n",
    "    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())] # Get the Prediction Score\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())] # Bounding boxes\n",
    "    pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1] # Get list of index with score greater than threshold.\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    pred_class = pred_class[:pred_t+1]\n",
    "    pred_scores =pred_score[:pred_t+1]\n",
    "    return pred_boxes, pred_class,pred_scores     \n",
    "    \n",
    "\n",
    "def format_predictions(boxes,classes,scores,im_id):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        if(float(scores[i] < 0.3)):\n",
    "            continue\n",
    "        # take only the pedestrian\n",
    "        cat_id = 1\n",
    "        if(classes[i] != 1): #1=person in the COCO dataset\n",
    "            continue\n",
    "        \n",
    "        x = {\n",
    "            \"image_id\": int(im_id),\n",
    "            \"category_id\": int(cat_id),\n",
    "            \"bbox\":[round(float(boxes[i][0]),5), round(float(boxes[i][1]),5),\n",
    "                    round(float(boxes[i][2] - boxes[i][0]),5) , round(float(boxes[i][3] -boxes[i][1]),5) ],\n",
    "            \"score\": round(float(scores[i]),5)\n",
    "            \n",
    "        }\n",
    "        predictions.append(x)\n",
    "        \n",
    "    return predictions\n",
    "        \n",
    "def format_output(output,ids):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i in range( len(output) ):\n",
    "\n",
    "        classes = output[i]['labels']\n",
    "        scores = output[i]['scores']\n",
    "        bboxes = output[i]['boxes']\n",
    "        \n",
    "        result.extend(format_predictions(bboxes,classes,scores,ids[i]))\n",
    "        \n",
    "    return result\n",
    "            \n",
    "        \n",
    "def main():\n",
    "    # Decide which device we want to run on\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "    print(\"Use device: \" + str(device))\n",
    "    \n",
    "    batch_size = 4\n",
    "    num_workers = 1\n",
    "\n",
    "    file_path = '/home/test/data/nightowls/nightowls_validation.json'\n",
    "    images_path = '/home/test/data/nightowls/nightowls_validation/'\n",
    "    output_file = './test.json'\n",
    "    \n",
    "    #dataset = MyCustomDataset(file_path,images_path)\n",
    "   \n",
    "    # Load data\n",
    "    data_loader = getDataLoader(file_path, images_path, batch_size, num_workers)\n",
    "    \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    json_output = []\n",
    "    \n",
    "    for batch_index, (data,ids) in enumerate(data_loader,0):\n",
    "        print('input:')\n",
    "        print(str(ids.data.cpu().numpy()))\n",
    "        dataaaa = data.to(device)\n",
    "        #lbl = lbl.to(device)\n",
    "        print('batch: ' + str(batch_index))\n",
    "        outputs = model(dataaaa)\n",
    "        \n",
    "        json_output.extend(format_output(outputs,ids))\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(json_output, f) \n",
    "        \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Original PennFudanDataset\n",
    "'''\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Training for the original PennFudanDataset\n",
    "'''\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn, drop_last=True)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn, drop_last=True)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "        \n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    #evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cuda:0\n",
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7040441])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7002887])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7002588])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7056881])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7001236])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7009671])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7027987])},)\n",
      "targets: ({'boxes': tensor([[-20., 246.,  15.,  33.]]), 'labels': tensor([4]), 'image_id': tensor([7020816])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7019763])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7039268])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7022185])},)\n",
      "targets: ({'boxes': tensor([[0., 0., 1., 1.]]), 'labels': tensor([1]), 'image_id': tensor([7036226])},)\n",
      "Loss is nan, stopping training\n",
      "{'loss_classifier': tensor(1.3277, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0., device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(15.0255, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Try to modify my dataset to be more similar to the previous one\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.nn import utils\n",
    "import transforms as T\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "from coco import COCO\n",
    "\n",
    "# annotation from the training set example:\n",
    "# {\"occluded\":false,\"difficult\":false,\"bbox\":[775,207,53,140],\"id\":1003556,\"category_id\":1,\"image_id\":1004817,\"pose_id\":2,\"tracking_id\":1000185,\"ignore\":0,\"area\":7420,\"truncated\":false}\n",
    "# bbox is [x,y,width,height]\n",
    "# there are four category: [1,2,3,4]\n",
    "\n",
    "'''\n",
    "Custom Dataset in order to load the images with the bbox and the labels\n",
    "'''\n",
    "class MyCustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, images_path, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.images_path = images_path\n",
    "        \n",
    "        # get the dataset, the annotiations and the imagesID\n",
    "        self.cocovar = COCO(file_path)\n",
    "        self.imgIds = sorted(self.cocovar.getImgIds())\n",
    "\n",
    "        self.imgs_count = len(self.imgIds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # get the image id and its filename\n",
    "        img_id = self.imgIds[index]\n",
    "        cocoimg = self.cocovar.loadImgs(ids=img_id)[0]\n",
    "        img_filename = cocoimg['file_name']\n",
    "\n",
    "        # Open image and apply the transformation\n",
    "        img_as_img = Image.open(os.path.join(self.images_path,img_filename))\n",
    "\n",
    "        #load the ids of the annotations for the images\n",
    "        image_annotations = self.cocovar.getAnnIds(img_id)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        area = []\n",
    "        occluded = []\n",
    "        difficult = []\n",
    "        pose_id = []\n",
    "        ignore = []\n",
    "        \n",
    "        num_objs = len(image_annotations)\n",
    "        \n",
    "        \n",
    "        # for every annotation\n",
    "        for ann_id in image_annotations:\n",
    "            #load all the annotation data\n",
    "            ann = self.cocovar.loadAnns(ids=ann_id)[0]\n",
    "            coordinates = []\n",
    "            for i in ann['bbox']:\n",
    "                coordinates.append(i)\n",
    "            \n",
    "            boxes.append(coordinates)\n",
    "            labels.append(ann['category_id'])\n",
    "            area.append(ann['area'])\n",
    "            occluded.append(ann['occluded'])\n",
    "            difficult.append(ann['difficult'])\n",
    "            pose_id.append(ann['pose_id'])\n",
    "            ignore.append(ann['ignore'])\n",
    "            \n",
    "        if(len(boxes)==0):\n",
    "            boxes = [[10,10,100,100]]\n",
    "            labels = [1]\n",
    "        \n",
    "            \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([img_id])\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        \n",
    "        img, target = self.transforms(img_as_img,target)\n",
    "            \n",
    "        #print('inside target: ' +str(target))\n",
    "        \n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imgs_count \n",
    "\n",
    "# Function that will create the dataset and the dataloader and return the dataloader\n",
    "def getDataLoader(file_path,images_path,batch_size,num_workers):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    transform = T.Compose(transforms)\n",
    "\n",
    "    dataset = MyCustomDataset(file_path,images_path,transform)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                              shuffle=True, num_workers=num_workers, drop_last=True,\n",
    "                                              collate_fn=utils.collate_fn)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "'''\n",
    "Import the model\n",
    "'''\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "        \n",
    "def main(batch_size,num_workers,n_classes,num_epochs):\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Use device: \" + str(device))\n",
    "\n",
    "    file_path = '/home/test/data/nightowls/nightowls_validation.json'\n",
    "    images_path = '/home/test/data/nightowls/nightowls_validation/'\n",
    "    output_file = './test.json'\n",
    "   \n",
    "    # Load data\n",
    "    data_loader = getDataLoader(file_path, images_path, batch_size, num_workers)\n",
    "    \n",
    "    # Load the model\n",
    "    model = get_instance_segmentation_model(n_classes)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # And a learning rate scheduler which decreases the learning rate by 10x every 30 epochs\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=30,gamma = 0.1)\n",
    "\n",
    "\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "    (imgs, targets) = next(iter(data_loader))\n",
    "    print('targets: ' + str(targets))\n",
    "\n",
    "    \n",
    "    '''\n",
    "    for i, (imgs,targets) in enumerate(data_loader):\n",
    "        print('batch ' + str(i))\n",
    "        print('targets: ' +str(targets))\n",
    "\n",
    "        for t in targets:\n",
    "            print('t:' + str(t) + ' type: ' + str(type(t)))\n",
    "        \n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    '''\n",
    "\n",
    "    # let's train it for some epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        #evaluate(model, data_loader_test, device=device)\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "n_classes = 4\n",
    "num_epochs = 2\n",
    "main(batch_size,num_workers,n_classes,num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
